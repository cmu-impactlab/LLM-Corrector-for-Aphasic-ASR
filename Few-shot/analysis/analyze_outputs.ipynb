{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Strategy Analysis for Outputs Folder\n",
    "\n",
    "This notebook automatically analyzes all strategies and runs in the `outputs` folder. It aggregates and visualizes metrics (WER, CER, SIM) for each strategy, run, and sentence grouping.\n",
    "\n",
    "- **Flexible:** Handles any number of strategies, runs, samples, and sentence groupings.\n",
    "- **Metrics:** WER, CER, SIM from `evaluation_metrics.csv`.\n",
    "- **Visualizations:** Line plots, bar plots, and more using matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Set the outputs directory\n",
    "outputs_dir = 'outputs'\n",
    "\n",
    "# Discover all strategies/runs\n",
    "strategies = [d for d in os.listdir(outputs_dir) if os.path.isdir(os.path.join(outputs_dir, d))]\n",
    "print(f'Found strategies/runs: {strategies}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure: strategy -> sentence_count -> metric_type -> list of values\n",
    "data = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "# Parse strategy names to extract strategy and run number\n",
    "def parse_strategy_name(strategy_name):\n",
    "    \"\"\"Parse strategy name like '414_data_driven_run1' into strategy and run\"\"\"\n",
    "    parts = strategy_name.split('_')\n",
    "    if len(parts) >= 3 and parts[-1].startswith('run'):\n",
    "        run_num = parts[-1]\n",
    "        strategy = '_'.join(parts[:-1])\n",
    "        return strategy, run_num\n",
    "    else:\n",
    "        return strategy_name, 'run1'\n",
    "\n",
    "# Collect all data\n",
    "for strategy_run in strategies:\n",
    "    strategy_path = os.path.join(outputs_dir, strategy_run)\n",
    "    strategy, run = parse_strategy_name(strategy_run)\n",
    "    \n",
    "    # Each strategy/run has multiple samples\n",
    "    if not os.path.isdir(strategy_path):\n",
    "        continue\n",
    "    \n",
    "    samples = [s for s in os.listdir(strategy_path) if os.path.isdir(os.path.join(strategy_path, s))]\n",
    "    \n",
    "    for sample in samples:\n",
    "        sample_path = os.path.join(strategy_path, sample)\n",
    "        # Each sample has folders like '2_sentences', '4_sentences', etc.\n",
    "        sentence_folders = [f for f in os.listdir(sample_path) if os.path.isdir(os.path.join(sample_path, f)) and '_sentences' in f]\n",
    "        \n",
    "        for sent_folder in sentence_folders:\n",
    "            sent_path = os.path.join(sample_path, sent_folder)\n",
    "            eval_file = os.path.join(sent_path, 'evaluation_metrics.csv')\n",
    "            \n",
    "            if os.path.exists(eval_file):\n",
    "                try:\n",
    "                    df = pd.read_csv(eval_file)\n",
    "                    # Extract number of sentences from folder name\n",
    "                    num_sent = int(sent_folder.split('_')[0])\n",
    "                    \n",
    "                    for _, row in df.iterrows():\n",
    "                        metric_type = row['Type']\n",
    "                        data[f'{strategy}_{run}'][num_sent][f'{metric_type}_WER'].append(row['WER'])\n",
    "                        data[f'{strategy}_{run}'][num_sent][f'{metric_type}_CER'].append(row['CER'])\n",
    "                        data[f'{strategy}_{run}'][num_sent][f'{metric_type}_SIM'].append(row['SIM'])\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f'Error reading {eval_file}: {e}')\n",
    "\n",
    "print(f\"\\\\nData collected for {len(data)} strategy/run combinations\")\n",
    "for strategy_run in data:\n",
    "    sentence_counts = sorted(data[strategy_run].keys())\n",
    "    print(f\"{strategy_run}: sentence counts {sentence_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics vs number of sentences for each strategy/run\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Define colors for different metrics\n",
    "colors = {'Raw ASR_WER': 'red', 'Improved_WER': 'darkred',\n",
    "          'Raw ASR_CER': 'blue', 'Improved_CER': 'darkblue', \n",
    "          'Raw ASR_SIM': 'green', 'Improved_SIM': 'darkgreen'}\n",
    "\n",
    "# Create subplots for each metric type\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['Raw ASR_WER', 'Improved_WER', 'Raw ASR_CER', 'Improved_CER', 'Raw ASR_SIM', 'Improved_SIM']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for strategy_run in data:\n",
    "        num_sents = sorted(data[strategy_run].keys())\n",
    "        means = []\n",
    "        stds = []\n",
    "        \n",
    "        for n in num_sents:\n",
    "            values = data[strategy_run][n].get(metric, [])\n",
    "            if values:\n",
    "                means.append(np.mean(values))\n",
    "                stds.append(np.std(values))\n",
    "            else:\n",
    "                means.append(np.nan)\n",
    "                stds.append(np.nan)\n",
    "        \n",
    "        # Filter out NaN values for plotting\n",
    "        valid_indices = ~np.isnan(means)\n",
    "        if np.any(valid_indices):\n",
    "            valid_sents = np.array(num_sents)[valid_indices]\n",
    "            valid_means = np.array(means)[valid_indices]\n",
    "            valid_stds = np.array(stds)[valid_indices]\n",
    "            \n",
    "            ax.errorbar(valid_sents, valid_means, yerr=valid_stds, \n",
    "                       label=strategy_run, marker='o', linestyle='-', linewidth=2)\n",
    "    \n",
    "    ax.set_title(f'{metric} vs Number of Sentences', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Number of Sentences')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Compare Strategies for a Specific Sentence Count\n",
    "\n",
    "You can compare all strategies for a specific sentence count below. Change `sentence_count_to_compare` as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_count_to_compare = 6  # Change as needed\n",
    "\n",
    "# Get all available sentence counts across all strategies\n",
    "all_sentence_counts = set()\n",
    "for strategy_run in data:\n",
    "    all_sentence_counts.update(data[strategy_run].keys())\n",
    "all_sentence_counts = sorted(all_sentence_counts)\n",
    "\n",
    "print(f\"Available sentence counts: {all_sentence_counts}\")\n",
    "print(f\"Comparing strategies for {sentence_count_to_compare} sentences\")\n",
    "\n",
    "# Create bar plots comparing strategies for the chosen sentence count\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['Raw ASR_WER', 'Improved_WER', 'Raw ASR_CER', 'Improved_CER', 'Raw ASR_SIM', 'Improved_SIM']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    strategy_names = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for strategy_run in data:\n",
    "        if sentence_count_to_compare in data[strategy_run]:\n",
    "            values = data[strategy_run][sentence_count_to_compare].get(metric, [])\n",
    "            if values:\n",
    "                strategy_names.append(strategy_run)\n",
    "                means.append(np.mean(values))\n",
    "                stds.append(np.std(values))\n",
    "    \n",
    "    if strategy_names:\n",
    "        bars = ax.bar(range(len(strategy_names)), means, yerr=stds, \n",
    "                     capsize=5, alpha=0.7, color=colors.get(metric, 'gray'))\n",
    "        ax.set_title(f'{metric} for {sentence_count_to_compare} Sentences', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_xlabel('Strategy/Run')\n",
    "        ax.set_xticks(range(len(strategy_names)))\n",
    "        ax.set_xticklabels(strategy_names, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, mean in zip(bars, means):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (max(means) * 0.01), \n",
    "                   f'{mean:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement percentages\n",
    "improvement_data = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for strategy_run in data:\n",
    "    for sentence_count in data[strategy_run]:\n",
    "        # WER improvement (lower is better, so improvement is negative change)\n",
    "        raw_wer = data[strategy_run][sentence_count].get('Raw ASR_WER', [])\n",
    "        imp_wer = data[strategy_run][sentence_count].get('Improved_WER', [])\n",
    "        if raw_wer and imp_wer:\n",
    "            wer_improvement = ((np.mean(raw_wer) - np.mean(imp_wer)) / np.mean(raw_wer)) * 100\n",
    "            improvement_data[strategy_run][sentence_count]['WER_improvement'] = wer_improvement\n",
    "        \n",
    "        # CER improvement (lower is better, so improvement is negative change)\n",
    "        raw_cer = data[strategy_run][sentence_count].get('Raw ASR_CER', [])\n",
    "        imp_cer = data[strategy_run][sentence_count].get('Improved_CER', [])\n",
    "        if raw_cer and imp_cer:\n",
    "            cer_improvement = ((np.mean(raw_cer) - np.mean(imp_cer)) / np.mean(raw_cer)) * 100\n",
    "            improvement_data[strategy_run][sentence_count]['CER_improvement'] = cer_improvement\n",
    "        \n",
    "        # SIM improvement (higher is better, so improvement is positive change)\n",
    "        raw_sim = data[strategy_run][sentence_count].get('Raw ASR_SIM', [])\n",
    "        imp_sim = data[strategy_run][sentence_count].get('Improved_SIM', [])\n",
    "        if raw_sim and imp_sim:\n",
    "            sim_improvement = ((np.mean(imp_sim) - np.mean(raw_sim)) / np.mean(raw_sim)) * 100\n",
    "            improvement_data[strategy_run][sentence_count]['SIM_improvement'] = sim_improvement\n",
    "\n",
    "# Plot improvement percentages\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "improvement_metrics = ['WER_improvement', 'CER_improvement', 'SIM_improvement']\n",
    "titles = ['WER Improvement (%)', 'CER Improvement (%)', 'SIM Improvement (%)']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(improvement_metrics, titles)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for strategy_run in improvement_data:\n",
    "        sentence_counts = sorted(improvement_data[strategy_run].keys())\n",
    "        improvements = []\n",
    "        \n",
    "        for sc in sentence_counts:\n",
    "            imp = improvement_data[strategy_run][sc].get(metric, np.nan)\n",
    "            improvements.append(imp)\n",
    "        \n",
    "        # Filter out NaN values\n",
    "        valid_indices = ~np.isnan(improvements)\n",
    "        if np.any(valid_indices):\n",
    "            valid_counts = np.array(sentence_counts)[valid_indices]\n",
    "            valid_improvements = np.array(improvements)[valid_indices]\n",
    "            \n",
    "            ax.plot(valid_counts, valid_improvements, marker='o', linewidth=2, \n",
    "                   label=strategy_run, markersize=6)\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Number of Sentences')\n",
    "    ax.set_ylabel('Improvement (%)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Let's create a summary table showing the average performance across all sentence counts for each strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n",
    "summary_stats = []\n",
    "\n",
    "for strategy_run in data:\n",
    "    all_raw_wer = []\n",
    "    all_imp_wer = []\n",
    "    all_raw_cer = []\n",
    "    all_imp_cer = []\n",
    "    all_raw_sim = []\n",
    "    all_imp_sim = []\n",
    "    \n",
    "    for sentence_count in data[strategy_run]:\n",
    "        all_raw_wer.extend(data[strategy_run][sentence_count].get('Raw ASR_WER', []))\n",
    "        all_imp_wer.extend(data[strategy_run][sentence_count].get('Improved_WER', []))\n",
    "        all_raw_cer.extend(data[strategy_run][sentence_count].get('Raw ASR_CER', []))\n",
    "        all_imp_cer.extend(data[strategy_run][sentence_count].get('Improved_CER', []))\n",
    "        all_raw_sim.extend(data[strategy_run][sentence_count].get('Raw ASR_SIM', []))\n",
    "        all_imp_sim.extend(data[strategy_run][sentence_count].get('Improved_SIM', []))\n",
    "    \n",
    "    summary_stats.append({\n",
    "        'Strategy/Run': strategy_run,\n",
    "        'Avg Raw WER': np.mean(all_raw_wer) if all_raw_wer else np.nan,\n",
    "        'Avg Improved WER': np.mean(all_imp_wer) if all_imp_wer else np.nan,\n",
    "        'Avg Raw CER': np.mean(all_raw_cer) if all_raw_cer else np.nan,\n",
    "        'Avg Improved CER': np.mean(all_imp_cer) if all_imp_cer else np.nan,\n",
    "        'Avg Raw SIM': np.mean(all_raw_sim) if all_raw_sim else np.nan,\n",
    "        'Avg Improved SIM': np.mean(all_imp_sim) if all_imp_sim else np.nan,\n",
    "        'WER Improvement %': ((np.mean(all_raw_wer) - np.mean(all_imp_wer)) / np.mean(all_raw_wer)) * 100 if all_raw_wer and all_imp_wer else np.nan,\n",
    "        'CER Improvement %': ((np.mean(all_raw_cer) - np.mean(all_imp_cer)) / np.mean(all_raw_cer)) * 100 if all_raw_cer and all_imp_cer else np.nan,\n",
    "        'SIM Improvement %': ((np.mean(all_imp_sim) - np.mean(all_raw_sim)) / np.mean(all_raw_sim)) * 100 if all_raw_sim and all_imp_sim else np.nan,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "print(\"Summary Statistics Across All Sentence Counts:\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.round(4))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Improvement Analysis\n",
    "\n",
    "Let's analyze the improvement (Raw ASR vs Improved) for each strategy and sentence count.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
